/**
 * LLM Client - Now using Ollama for local inference
 * 
 * This module provides chat generation and embeddings using local Ollama service
 * instead of cloud-based Gemini API.
 */

import { getCrisisPromptAddition } from "@/lib/safety/crisis-detection";
import {
  generateText as ollamaGenerateText,
  generateEmbedding as ollamaGenerateEmbedding,
  checkOllamaHealth,
  ollamaConfig
} from "@/lib/ollama/client";

// Export Ollama config for other modules
export { ollamaConfig };

/**
 * Check if LLM service is available
 */
export async function checkLLMHealth(): Promise<boolean> {
  return await checkOllamaHealth();
}

// Generate text using Ollama with context
export async function generateWithContext(
  userMessage: string,
  contextChunks: Array<{ text: string; metadata: any }>,
  conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }> = [],
  fitbitData?: any,
  isCrisis: boolean = false,
  userMemories?: Array<{ id: string; memory: string; category?: string; score: number }>,
  aiHealthInsights?: any // AI-generated health analysis
) {
  // Build context from retrieved chunks
  const contextText = contextChunks
    .map((chunk, idx) => `[Context ${idx + 1}]\n${chunk.text}`)
    .join('\n\n');

  // Build conversation history (last 6 messages for better context)
  const recentHistory = conversationHistory.slice(-6);
  const historyText = recentHistory.length > 0
    ? recentHistory.map(msg => `${msg.role === 'user' ? 'Student' : 'Assistant'}: ${msg.content}`).join('\n\n')
    : '';

  // Build user memory context from Mem0 - Enhanced for personalization
  let memoryContext = '';
  if (userMemories && userMemories.length > 0) {
    memoryContext = userMemories
      .map((mem, idx) => {
        const categoryLabel = mem.category ? `[${mem.category.toUpperCase()}]` : '[GENERAL]';
        return `â€¢ ${categoryLabel} ${mem.memory}`;
      })
      .join('\n');
    memoryContext += '\n\n**Note**: Use these memories to provide personalized support. Reference their name, past concerns, preferences, and patterns naturally in your response.';
  }

  // Build Fitbit health data context
  let healthDataText = '';
  if (fitbitData?.connected && fitbitData?.recentData?.length > 0) {
    healthDataText = '\n## Student\'s Health Metrics (Last 7 Days):\n';

    // Organize by data type
    const activityData = fitbitData.recentData.filter((d: any) => d.type === 'activity');
    const heartRateData = fitbitData.recentData.filter((d: any) => d.type === 'heartrate');
    const sleepData = fitbitData.recentData.filter((d: any) => d.type === 'sleep');

    if (activityData.length > 0) {
      healthDataText += '\n### Activity Levels:\n';
      activityData.forEach((d: any) => {
        healthDataText += `- ${d.date}: ${d.data.steps} steps, ${d.data.activeMinutes} active minutes, ${d.data.calories} calories\n`;
      });
    }

    if (heartRateData.length > 0) {
      healthDataText += '\n### Heart Rate:\n';
      heartRateData.forEach((d: any) => {
        healthDataText += `- ${d.date}: Resting HR ${d.data.restingHeartRate} bpm\n`;
      });
    }

    if (sleepData.length > 0) {
      healthDataText += '\n### Sleep Patterns:\n';
      sleepData.forEach((d: any) => {
        const hours = Math.floor(d.data.minutesAsleep / 60);
        const minutes = d.data.minutesAsleep % 60;
        healthDataText += `- ${d.date}: ${hours}h ${minutes}m sleep (${d.data.efficiency}% efficiency)\n`;
      });
    }

    healthDataText += '\n**Analysis Instructions**: When relevant to the student\'s query, correlate their health metrics with their mental state. For example:\n';
    healthDataText += '- Low sleep + stress query = suggest sleep improvement strategies\n';
    healthDataText += '- Low activity + low mood = recommend gentle exercise\n';
    healthDataText += '- High resting heart rate + anxiety = discuss relaxation techniques\n';
    healthDataText += 'Only mention health data if it\'s directly relevant to their question. Don\'t force correlations.\n';
  }

  // Add AI-powered health analysis if available
  if (aiHealthInsights) {
    healthDataText += '\n### ðŸ¤– AI Health Analysis:\n';
    healthDataText += `**Summary:** ${aiHealthInsights.summary}\n`;
    healthDataText += `**Mental Health Impact:** ${aiHealthInsights.mentalHealthCorrelation}\n`;
    healthDataText += `**Urgency Level:** ${aiHealthInsights.urgencyLevel.toUpperCase()}\n`;

    if (aiHealthInsights.patterns && aiHealthInsights.patterns.length > 0) {
      healthDataText += `**Patterns Detected:** ${aiHealthInsights.patterns.join(', ')}\n`;
    }

    if (aiHealthInsights.recommendations && aiHealthInsights.recommendations.length > 0) {
      healthDataText += `**AI Recommendations:** ${aiHealthInsights.recommendations.join(' | ')}\n`;
    }

    healthDataText += '\n**Important**: This analysis was generated by a specialized AI model. Use these insights to provide personalized, evidence-based support.\n';
  }

  // Add crisis detection prompt if needed
  const crisisAddition = isCrisis ? getCrisisPromptAddition() : '';

  // Smart context truncation - keep most relevant parts
  const maxContextLength = 2000; // Increased from 500
  const maxHealthLength = 800;   // Increased from 300
  const maxHistoryLength = 600;  // Keep more history

  const truncatedContext = contextText.length > maxContextLength
    ? contextText.substring(0, maxContextLength) + '...[more available]'
    : contextText;

  const truncatedHealth = healthDataText.length > maxHealthLength
    ? healthDataText.substring(0, maxHealthLength) + '...[summary truncated]'
    : healthDataText;

  const truncatedHistory = historyText.length > maxHistoryLength
    ? historyText.substring(historyText.length - maxHistoryLength)
    : historyText;

  // Create the prompt - optimized for local model (med-assistant/deepseek-r1)
  const prompt = `You are a supportive mental health companion for college students at RVCE. Be warm, empathetic, and concise.

## Your Role:
- Provide emotional support and evidence-based mental health guidance
- Remember and reference past conversations to provide personalized support
- Use health data insights when relevant to the conversation
- Always prioritize the student's well-being
${crisisAddition}
${memoryContext ? `\n## What I Remember About This Student (Use these to personalize your response):\n${memoryContext}\n` : ''}
${truncatedContext ? `\n## Relevant Mental Health Knowledge:\n${truncatedContext}\n` : ''}
${truncatedHealth ? `\n## Student's Health Data:\n${truncatedHealth}\n` : ''}
${truncatedHistory ? `\n## Recent Conversation:\n${truncatedHistory}\n` : ''}
## Student's Current Message:
${userMessage}

## Response Guidelines:
1. Keep response under 150 words, be concise
2. Use short paragraphs (2-3 lines max)
3. Use **bold** for key points sparingly
4. Use bullet points (â€¢) for actionable tips
5. Be conversational, warm, and supportive
6. **Reference their past conversations and preferences** if available in memories
7. Correlate health data with their concern if relevant
8. Only suggest professional help when truly necessary
9. If RAG context is provided, incorporate that knowledge naturally

Your response:`;

  try {
    console.log('ðŸ”µ OLLAMA: Generating response with local model...');
    const text = await ollamaGenerateText(prompt, {
      temperature: 0.7,
      maxTokens: 2048,
    });
    console.log('ðŸŸ¢ OLLAMA: Response generated successfully');

    return text;
  } catch (error: any) {
    console.error('ðŸ”´ OLLAMA: API error:', {
      message: error.message,
      details: error
    });

    // Provide helpful error messages
    if (error.message?.includes('not running') || error.message?.includes('ECONNREFUSED')) {
      throw new Error('Ollama service is not running. Please start Ollama with: ollama serve');
    } else if (error.message?.includes('timeout')) {
      throw new Error('Response generation timed out. The model may be loading or overloaded.');
    } else if (error.message?.includes('not found')) {
      throw new Error(`Model not found. Please pull it with: ollama pull ${ollamaConfig.chatModel}`);
    }

    throw new Error(`Failed to generate response: ${error.message}`);
  }
}

// Generate embeddings using Ollama's embedding model
export async function generateEmbedding(text: string): Promise<number[]> {
  try {
    console.log('ðŸ”µ OLLAMA: Generating embedding...');
    const embedding = await ollamaGenerateEmbedding(text);
    console.log(`âœ… Embedding generated (${embedding.length} dimensions)`);
    return embedding;
  } catch (error: any) {
    console.error('ðŸ”´ OLLAMA: Embedding error:', error.message);

    if (error.message?.includes('not running') || error.message?.includes('ECONNREFUSED')) {
      throw new Error('Ollama service is not running. Please start Ollama with: ollama serve');
    } else if (error.message?.includes('not found')) {
      throw new Error(`Embedding model not found. Please pull it with: ollama pull ${ollamaConfig.embedModel}`);
    }

    throw new Error(`Failed to generate embedding: ${error.message}`);
  }
}